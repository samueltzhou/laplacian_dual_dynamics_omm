{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e2ffcd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-28 14:57:31.754790: W external/xla/xla/service/gpu/nvptx_compiler.cc:763] The NVIDIA driver's CUDA version is 12.4 which is older than the ptxas CUDA version (12.9.86). Because the driver is older than the ptxas version, XLA is disabling parallel compilation, which may slow down compilation. You should update your NVIDIA driver or use the NVIDIA-provided CUDA forward compatibility packages.\n"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "key = jax.random.key(69)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "18376b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "B = 32\n",
    "d = 11\n",
    "FROBENIUS_NORM_GAIN = 6.9\n",
    "FROBENIUS_NORM_BIAS = 4.2\n",
    "\n",
    "start = jax.random.normal(key, (B, d))\n",
    "end = jax.random.normal(key, (B, d))\n",
    "\n",
    "# create masks\n",
    "coeff_vector_mask = jnp.arange(d, 0, -1)\n",
    "coeff_vector_mask = (\n",
    "    coeff_vector_mask**2\n",
    ")  # this is just a test, comment this out later\n",
    "coeff_vector_mask_col = jnp.expand_dims(coeff_vector_mask, 1)  # Shape: (d, 1)\n",
    "coeff_vector_mask_row = jnp.expand_dims(coeff_vector_mask, 0)  # Shape: (1, d)\n",
    "coeff_matrix_mask = jnp.minimum(\n",
    "    coeff_vector_mask_col, coeff_vector_mask_row\n",
    ")  # Shape: (d, d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ffe3ed0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create masks\n",
    "coeff_vector_mask = jnp.arange(d, 0, -1)\n",
    "coeff_vector_mask = (\n",
    "    coeff_vector_mask**2\n",
    ")  # this is just a test, comment this out later\n",
    "coeff_vector_mask_col = jnp.expand_dims(coeff_vector_mask, 1)  # Shape: (d, 1)\n",
    "coeff_vector_mask_row = jnp.expand_dims(coeff_vector_mask, 0)  # Shape: (1, d)\n",
    "coeff_matrix_mask = jnp.minimum(\n",
    "    coeff_vector_mask_col, coeff_vector_mask_row\n",
    ")  # Shape: (d, d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a91426fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "old loss computation: -364.30926513671875\n"
     ]
    }
   ],
   "source": [
    "### old implementation of loss\n",
    "averaged_inner_prod = (\n",
    "    jnp.einsum(\"bi, bi -> i\", start, end)\n",
    "    / start.shape[0]\n",
    ")\n",
    "approximation_error_loss = -2 * jnp.sum(averaged_inner_prod * coeff_vector_mask)\n",
    "\n",
    "product_1 = jnp.einsum(\n",
    "    \"bj, bk -> bjk\", start, end\n",
    ")\n",
    "averaged_product_1 = jnp.mean(product_1, axis=0)\n",
    "product_2 = jnp.einsum(\n",
    "    \"bj, bk -> bjk\", start, start\n",
    ")\n",
    "averaged_product_2 = jnp.mean(product_2, axis=0)\n",
    "orthogonality_loss = jnp.sum(\n",
    "    coeff_matrix_mask * averaged_product_1 * averaged_product_2\n",
    ")\n",
    "\n",
    "loss = approximation_error_loss + orthogonality_loss\n",
    "print(f\"old loss computation: {loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "508b838a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new loss computation: -364.3092041015625\n"
     ]
    }
   ],
   "source": [
    "### new implementation of loss\n",
    "cov_matrix = start.T @ start / start.shape[0]\n",
    "corr_matrix = start.T @ end / start.shape[0]\n",
    "scaled_corr_matrix = coeff_matrix_mask * corr_matrix\n",
    "\n",
    "raw_loss = -2 * jnp.trace(scaled_corr_matrix) + jnp.sum(cov_matrix * scaled_corr_matrix)\n",
    "\n",
    "print(f\"new loss computation: {raw_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2fb36e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_approximation_error_loss(\n",
    "        start_representation, end_representation\n",
    "    ):\n",
    "    \"\"\"\n",
    "    Compute the approximation error loss between the start and end representations.\n",
    "    This is the -2 Sigma <g_l, Tf_l> term, but as we are working with EVD, it is just <f_l, Tf_l>.\n",
    "    Recall from the derivation that this is equivalent to E[(f_l - Tf_l)^2], where we sample over\n",
    "    transitions and take an expectation over p(x, x').\n",
    "\n",
    "    Args:\n",
    "        start_representation: The representation of the start state.\n",
    "        end_representation: The representation of the end state.\n",
    "\n",
    "    Returns:\n",
    "        The approximation error loss.\n",
    "    \"\"\"\n",
    "    print(\n",
    "        f\"for approximation error loss: start_representation: {start_representation.shape}\"\n",
    "    )\n",
    "    print(\n",
    "        f\"for approximation error loss: end_representation: {end_representation.shape}\"\n",
    "    )\n",
    "    # Compute the loss term\n",
    "    # loss = -2 * ((start_representation - end_representation)**2).mean() # need to recheck this...\n",
    "    loss = 0\n",
    "    coeff_mask = jnp.arange(d, 0, -1)  # for joint LoRA\n",
    "    # coeff_mask = coeff_mask**2  # this is just a test, comment this out later\n",
    "    if len(start_representation.shape) == 1 and len(end_representation.shape) == 1:\n",
    "        print(\"one dimensional approximation error loss\")\n",
    "\n",
    "        # computing loss via squares\n",
    "        squared_diff = (start_representation - end_representation) ** 2\n",
    "        joint_squared_diff = squared_diff * coeff_mask\n",
    "        loss = -(jnp.sum(joint_squared_diff))\n",
    "\n",
    "        # computing loss via straight inner product\n",
    "        inner_prod = jnp.dot(start_representation, end_representation)\n",
    "        loss = -2 * inner_prod\n",
    "    else:\n",
    "        print(\"batched approximation error loss\")\n",
    "        # shapes (1024, 11) and (1024, 11)\n",
    "\n",
    "        # computing loss via squares\n",
    "        # squared_diff = (start_representation - end_representation) ** 2\n",
    "        # joint_squared_diff = squared_diff * coeff_mask\n",
    "        # loss = - jnp.mean(jnp.sum(joint_squared_diff, axis=1))\n",
    "\n",
    "        # computing loss via straight inner product\n",
    "        # inner_prod = jnp.einsum('bi, bi, i -> b', start_representation, end_representation, coeff_mask)\n",
    "        # loss = -2 * jnp.mean(inner_prod)\n",
    "\n",
    "        averaged_inner_prod = (\n",
    "            jnp.einsum(\"bi, bi -> i\", start_representation, end_representation)\n",
    "            / start_representation.shape[0]\n",
    "        )\n",
    "        loss = -2 * jnp.sum(averaged_inner_prod * coeff_mask)\n",
    "\n",
    "\n",
    "    return loss\n",
    "\n",
    "def compute_orthogonality_loss(\n",
    "    representation_1, representation_2, representation_2_end\n",
    "):\n",
    "    \"\"\"\n",
    "    Compute the orthogonality loss between the two representations.\n",
    "\n",
    "    If we are working with LoRA:\n",
    "        This is the Sigma_l Sigma_l' <f_l | f_l'> <g_l | g_l'> term.\n",
    "        Since we are working with EVD, it is just Sigma_l <f_l | f_l'>^2.\n",
    "    If we are working with OMM:\n",
    "        This is Sigma_l Sigma_l' <f_l | Tf_l'> <f_l | f_l'> term.\n",
    "\n",
    "    Args:\n",
    "        representation_1: The first representation.\n",
    "        representation_2: The second representation.\n",
    "\n",
    "    Returns:\n",
    "        The orthogonality loss.\n",
    "    \"\"\"\n",
    "    loss = 0\n",
    "\n",
    "    print(f\"for orthogonality loss: representation_1: {representation_1.shape}\")\n",
    "    print(f\"for orthogonality loss: representation_2: {representation_2.shape}\")\n",
    "\n",
    "    # Create the mask in a vectorized way\n",
    "    coeff_vector_mask = jnp.arange(d, 0, -1)\n",
    "    # coeff_vector_mask = (\n",
    "    #     coeff_vector_mask**2\n",
    "    # )  # this is just a test, comment this out later\n",
    "    coeff_vector_mask_col = jnp.expand_dims(coeff_vector_mask, 1)  # Shape: (d, 1)\n",
    "    coeff_vector_mask_row = jnp.expand_dims(coeff_vector_mask, 0)  # Shape: (1, d)\n",
    "    coeff_matrix_mask = jnp.minimum(\n",
    "        coeff_vector_mask_col, coeff_vector_mask_row\n",
    "    )  # Shape: (d, d)\n",
    "\n",
    "    # OMM loss case: Sigma_l Sigma_l' <f_l | Tf_l'> <f_l | f_l'>\n",
    "    try:\n",
    "        if len(representation_1.shape) == 1:\n",
    "            print(\"one dimensional orthogonality loss\")\n",
    "            product_1 = jnp.einsum(\n",
    "                \"j, k -> jk\", representation_2, representation_2_end\n",
    "            )\n",
    "            product_2 = jnp.einsum(\n",
    "                \"j, k -> jk\", representation_1, representation_1\n",
    "            )\n",
    "            loss += jnp.sum(coeff_matrix_mask * product_1 * product_2)\n",
    "        else:\n",
    "            print(\"batched orthogonality loss\")\n",
    "            product_1 = jnp.einsum(\n",
    "                \"bj, bk -> bjk\", representation_2, representation_2_end\n",
    "            )\n",
    "            averaged_product_1 = jnp.mean(product_1, axis=0)\n",
    "            product_2 = jnp.einsum(\n",
    "                \"bj, bk -> bjk\", representation_1, representation_1\n",
    "            )\n",
    "            averaged_product_2 = jnp.mean(product_2, axis=0)\n",
    "            loss += jnp.sum(\n",
    "                coeff_matrix_mask * averaged_product_1 * averaged_product_2\n",
    "            )\n",
    "    except:\n",
    "        print(f\"Shape of representation_1: {representation_1.shape}\")\n",
    "        print(f\"Shape of representation_2: {representation_2.shape}\")\n",
    "        raise\n",
    "    \n",
    "\n",
    "    return loss\n",
    "\n",
    "def compute_frobenius_norm_loss(representation, matrix_mask):\n",
    "    \"\"\"\n",
    "    Computes the Frobenius norm loss between the representation and the identity matrix.\n",
    "    \n",
    "    This is ||rep.T @ rep - I||_2. Bias factor removed, bias is to be multiplied in during the\n",
    "    actual loss computation.\n",
    "    \"\"\"\n",
    "    return jnp.sum(\n",
    "        matrix_mask\n",
    "        * (\n",
    "            representation.T @ representation / representation.shape[0]\n",
    "            - jnp.eye(representation.shape[1])\n",
    "        )\n",
    "        ** 2\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "24520409",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40.224\n",
      "-12.485597\n",
      "-32.225197\n",
      "17.889158\n",
      "14.044348\n",
      "6.9094186\n"
     ]
    }
   ],
   "source": [
    "key = jax.random.PRNGKey(696969)\n",
    "start_representation = jax.random.normal(jax.random.PRNGKey(0), (32, 11))\n",
    "end_representation = jax.random.normal(jax.random.PRNGKey(1), (32, 11))\n",
    "start_representation_2 = jax.random.normal(jax.random.PRNGKey(2), (32, 11))\n",
    "end_representation_2 = jax.random.normal(jax.random.PRNGKey(3), (32, 11))\n",
    "constraint_start_representation = jax.random.normal(jax.random.PRNGKey(4), (32, 11))\n",
    "constraint_end_representation = jax.random.normal(jax.random.PRNGKey(5), (32, 11))\n",
    "print(start_representation.sum())\n",
    "print(end_representation.sum())\n",
    "print(start_representation_2.sum())\n",
    "print(end_representation_2.sum())\n",
    "print(constraint_start_representation.sum())\n",
    "print(constraint_end_representation.sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "589917ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for approximation error loss: start_representation: (32, 11)\n",
      "for approximation error loss: end_representation: (32, 11)\n",
      "batched approximation error loss\n",
      "for orthogonality loss: representation_1: (32, 11)\n",
      "for orthogonality loss: representation_2: (32, 11)\n",
      "batched orthogonality loss\n",
      "132.10524\n"
     ]
    }
   ],
   "source": [
    "### old loss computation\n",
    "# Create the mask in a vectorized way\n",
    "coeff_vector_mask = jnp.arange(d, 0, -1)\n",
    "# coeff_vector_mask = (\n",
    "#     coeff_vector_mask**2\n",
    "# )  # this is just a test, comment this out later\n",
    "coeff_vector_mask_col = jnp.expand_dims(coeff_vector_mask, 1)  # Shape: (d, 1)\n",
    "coeff_vector_mask_row = jnp.expand_dims(coeff_vector_mask, 0)  # Shape: (1, d)\n",
    "coeff_matrix_mask = jnp.minimum(\n",
    "    coeff_vector_mask_col, coeff_vector_mask_row\n",
    ")  # Shape: (d, d)\n",
    "\n",
    "# Compute graph loss and regularization\n",
    "approximation_error_loss = compute_approximation_error_loss(\n",
    "    start_representation, end_representation\n",
    ")\n",
    "orthogonality_loss = compute_orthogonality_loss(\n",
    "    constraint_start_representation,\n",
    "    start_representation_2,\n",
    "    end_representation_2,\n",
    ")\n",
    "frobenius_norm_loss = compute_frobenius_norm_loss(\n",
    "    constraint_end_representation, # note: beforehand we were using start_representation. is there an important difference in sampling?\n",
    "    coeff_matrix_mask,\n",
    ")\n",
    "\n",
    "old_loss = (\n",
    "    FROBENIUS_NORM_GAIN * (approximation_error_loss + orthogonality_loss) +\n",
    "    FROBENIUS_NORM_BIAS * frobenius_norm_loss\n",
    ")\n",
    "print(old_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d975447a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "132.10522\n"
     ]
    }
   ],
   "source": [
    "### new loss computation\n",
    "# Create the mask in a vectorized way\n",
    "coeff_vector_mask = jnp.arange(d, 0, -1)\n",
    "coeff_matrix_mask = jnp.minimum(\n",
    "    jnp.expand_dims(coeff_vector_mask, 1), jnp.expand_dims(coeff_vector_mask, 0)\n",
    ")  # Shape: (d, d)\n",
    "\n",
    "# V^T V\n",
    "cov_matrix = constraint_start_representation.T @ constraint_start_representation / constraint_start_representation.shape[0] # [d, d]\n",
    "\n",
    "# V^T AV\n",
    "corr_matrix = start_representation.T @ end_representation / start_representation.shape[0] # [d, d]\n",
    "corr_matrix_2 = start_representation_2.T @ end_representation_2 / start_representation_2.shape[0] # [d, d]\n",
    "scaled_corr_matrix = coeff_matrix_mask * corr_matrix\n",
    "scaled_corr_matrix_2 = coeff_matrix_mask * corr_matrix_2\n",
    "\n",
    "# loss: trace(coeff_matrix_mask * (-2 corr_matrix + cov_matrix @ corr_matrix))\n",
    "raw_loss = -2 * jnp.trace(scaled_corr_matrix) + jnp.sum(cov_matrix * scaled_corr_matrix_2)\n",
    "\n",
    "# shift term (decorrelate this too)\n",
    "frobenius_norm_loss = compute_frobenius_norm_loss(\n",
    "    constraint_end_representation,\n",
    "    coeff_matrix_mask,\n",
    ")\n",
    "\n",
    "# Compute total loss. NOTE: WITH NON-DEFAULT BIAS AND GAIN, THIS HAS TO BE WITH ORBITALS\n",
    "loss = FROBENIUS_NORM_GAIN * raw_loss + FROBENIUS_NORM_BIAS * frobenius_norm_loss\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b7c2b3e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "883047.0\n",
      "883047.0\n"
     ]
    }
   ],
   "source": [
    "### check if matrix mask can be applied naively to frobenius norm loss\n",
    "# || V^TV - I ||_2\n",
    "# V should be (B, d)\n",
    "small_mask = jnp.array(\n",
    "    [[3, 2, 1],\n",
    "     [2, 2, 1],\n",
    "     [1, 1, 1]]\n",
    ")\n",
    "V = jnp.array([[1, 2, 3], [4, 5, 6], [7, 8, 9], [10, 11, 12], [3.15, 2.9, 8.1], [12.2, -6.9, 1.8]]) # (6, 3)\n",
    "I = jnp.eye(3)\n",
    "\n",
    "# brute force\n",
    "brute_force_loss = 0\n",
    "for i in range(1, 4):\n",
    "    brute_force_loss += jnp.sum((V[:, :i].T @ V[:, :i] - I[:i, :i]) ** 2)\n",
    "print(brute_force_loss)\n",
    "\n",
    "# vectorized\n",
    "print(jnp.sum(small_mask * (V.T @ V - I) ** 2))\n",
    "\n",
    "# holy shit these aren't equal im a retard\n",
    "# wait no these are equal i am NOT a retard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fc241338",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _compute_frobenius_norm_loss(representation):\n",
    "    \"\"\"\n",
    "    Compute the Frobenius norm loss between the representation and the identity matrix.\n",
    "\n",
    "    Args:\n",
    "        representation: The representation of the state. Shape (B, d)\n",
    "        alpha: The alpha parameter.\n",
    "\n",
    "    Returns:\n",
    "        The Frobenius norm loss.\n",
    "    \"\"\"\n",
    "    return jnp.sum(\n",
    "        (\n",
    "            representation.T @ representation / representation.shape[0]\n",
    "            - jnp.eye(representation.shape[1])\n",
    "        )\n",
    "        ** 2\n",
    "    )\n",
    "\n",
    "def compute_approximation_error_loss(\n",
    "    start_representation, end_representation\n",
    "):\n",
    "    \"\"\"\n",
    "    Compute the approximation error loss between the start and end representations.\n",
    "    This is the -2 Sigma <g_l, Tf_l> term, but as we are working with EVD, it is just <f_l, Tf_l>.\n",
    "    Recall from the derivation that this is equivalent to E[(f_l - Tf_l)^2], where we sample over\n",
    "    transitions and take an expectation over p(x, x').\n",
    "\n",
    "    Args:\n",
    "        start_representation: The representation of the start state.\n",
    "        end_representation: The representation of the end state.\n",
    "\n",
    "    Returns:\n",
    "        The approximation error loss.\n",
    "    \"\"\"\n",
    "    print(\n",
    "        f\"for approximation error loss: start_representation: {start_representation.shape}\"\n",
    "    )\n",
    "    print(\n",
    "        f\"for approximation error loss: end_representation: {end_representation.shape}\"\n",
    "    )\n",
    "    # Compute the loss term\n",
    "    # loss = -2 * ((start_representation - end_representation)**2).mean() # need to recheck this...\n",
    "    loss = 0\n",
    "    if len(start_representation.shape) == 1 and len(end_representation.shape) == 1:\n",
    "        print(\"one dimensional approximation error loss\")\n",
    "\n",
    "        # computing loss via squares\n",
    "        # squared_diff = (start_representation - end_representation) ** 2\n",
    "        # joint_squared_diff = squared_diff\n",
    "        # loss = -(jnp.sum(joint_squared_diff))\n",
    "\n",
    "        # computing loss via straight inner product\n",
    "        inner_prod = jnp.dot(start_representation, end_representation)\n",
    "        loss = -2 * inner_prod\n",
    "    else:\n",
    "        print(\"batched approximation error loss\")\n",
    "        # shapes (1024, 11) and (1024, 11)\n",
    "\n",
    "        averaged_inner_prod = (\n",
    "            jnp.einsum(\"bi, bi -> i\", start_representation, end_representation)\n",
    "            / start_representation.shape[0]\n",
    "        )\n",
    "        loss = -2 * jnp.sum(averaged_inner_prod)\n",
    "\n",
    "    return loss\n",
    "\n",
    "def compute_orthogonality_loss(\n",
    "    representation_1, representation_2, representation_2_end=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Compute the orthogonality loss between the two representations.\n",
    "\n",
    "    If we are working with LoRA:\n",
    "        This is the Sigma_l Sigma_l' <f_l | f_l'> <g_l | g_l'> term.\n",
    "        Since we are working with EVD, it is just Sigma_l <f_l | f_l'>^2.\n",
    "    If we are working with OMM:\n",
    "        This is Sigma_l Sigma_l' <f_l | Tf_l'> <f_l | f_l'> term.\n",
    "\n",
    "    Args:\n",
    "        representation_1: The first representation.\n",
    "        representation_2: The second representation.\n",
    "\n",
    "    Returns:\n",
    "        The orthogonality loss.\n",
    "    \"\"\"\n",
    "    loss = 0\n",
    "\n",
    "    # OMM loss case: Sigma_l Sigma_l' <f_l | Tf_l'> <f_l | f_l'>\n",
    "    try:\n",
    "        if len(representation_1.shape) == 1:\n",
    "            print(\"one dimensional orthogonality loss\")\n",
    "            product_1 = jnp.einsum(\n",
    "                \"j, k -> jk\", representation_2, representation_2_end\n",
    "            )\n",
    "            product_2 = jnp.einsum(\n",
    "                \"j, k -> jk\", representation_1, representation_1\n",
    "            )\n",
    "            loss += jnp.sum(product_1 * product_2)\n",
    "        else:\n",
    "            print(\"batched orthogonality loss\")\n",
    "            product_1 = (\n",
    "                jnp.einsum(\n",
    "                    \"bj, bk -> jk\", representation_2, representation_2_end\n",
    "                )\n",
    "                / representation_2.shape[0]\n",
    "            )\n",
    "            product_2 = (\n",
    "                jnp.einsum(\"bj, bk -> jk\", representation_1, representation_1)\n",
    "                / representation_1.shape[0]\n",
    "            )\n",
    "            loss += jnp.sum(product_1 * product_2)\n",
    "    except:\n",
    "        print(f\"Shape of representation_1: {representation_1.shape}\")\n",
    "        print(f\"Shape of representation_2: {representation_2.shape}\")\n",
    "        raise\n",
    "\n",
    "    return loss\n",
    "\n",
    "def _build_stop_grad_encoding(\n",
    "    encoding: jnp.ndarray, top_i: int, **kwargs\n",
    ") -> jnp.ndarray:\n",
    "    \"\"\"\n",
    "    Builds a top_i state encoding where everything but the top_ith eigenfunction is frozen.\n",
    "\n",
    "    Args:\n",
    "        encodings: The state encoding.\n",
    "        top_i: How many of the top eigenfunctions to use for the loss function.\n",
    "\n",
    "    Returns:\n",
    "        The top_i state encoding.\n",
    "    \"\"\"\n",
    "    # encodings are either of shape (d,) or (b, d)\n",
    "    if len(encoding.shape) == 1:\n",
    "        mask_function = lambda encoding: jnp.concatenate(\n",
    "            [\n",
    "                jax.lax.stop_gradient(encoding[: top_i - 1]),\n",
    "                encoding[top_i - 1 : top_i],\n",
    "            ],\n",
    "            axis=0,\n",
    "        )\n",
    "    else:\n",
    "        mask_function = lambda encoding: jnp.concatenate(\n",
    "            [\n",
    "                jax.lax.stop_gradient(encoding[:, : top_i - 1]),\n",
    "                encoding[:, top_i - 1 : top_i],\n",
    "            ],\n",
    "            axis=1,\n",
    "        )\n",
    "\n",
    "    return mask_function(encoding)\n",
    "\n",
    "def _compute_loss_function_component(top_i):\n",
    "    \"\"\"\n",
    "    Computes a single component of the loss function (the top_i loss).\n",
    "\n",
    "    Args:\n",
    "        params: The parameters of the model.\n",
    "        state_encoding: The state encoding.\n",
    "        top_i: How many of the top eigenfunctions to use for the loss function.\n",
    "\n",
    "    Returns:\n",
    "        Tuple of (loss, approximation_error_loss, orthogonality_loss)\n",
    "    \"\"\"\n",
    "\n",
    "    approximation_error_loss = compute_approximation_error_loss(\n",
    "        _build_stop_grad_encoding(start_representation, top_i),\n",
    "        _build_stop_grad_encoding(end_representation, top_i),\n",
    "    )\n",
    "    orthogonality_loss = compute_orthogonality_loss(\n",
    "        _build_stop_grad_encoding(constraint_start_representation, top_i),\n",
    "        (\n",
    "            _build_stop_grad_encoding(start_representation_2, top_i)\n",
    "        ),\n",
    "        representation_2_end=(\n",
    "            _build_stop_grad_encoding(end_representation_2, top_i)\n",
    "        ),\n",
    "    )\n",
    "    frobenius_norm_loss = _compute_frobenius_norm_loss(\n",
    "        _build_stop_grad_encoding(start_representation, top_i),\n",
    "    )\n",
    "    loss = approximation_error_loss + orthogonality_loss + FROBENIUS_NORM_BIAS * frobenius_norm_loss\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5b731bdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40.224\n",
      "-12.485597\n",
      "-32.225197\n",
      "17.889158\n",
      "14.044348\n",
      "6.9094186\n"
     ]
    }
   ],
   "source": [
    "B = 32\n",
    "d = 11\n",
    "FROBENIUS_NORM_GAIN = 1.0\n",
    "FROBENIUS_NORM_BIAS = 1.0\n",
    "# create masks\n",
    "coeff_vector_mask = jnp.arange(d, 0, -1)\n",
    "coeff_vector_mask = (\n",
    "    coeff_vector_mask**2\n",
    ")  # this is just a test, comment this out later\n",
    "coeff_vector_mask_col = jnp.expand_dims(coeff_vector_mask, 1)  # Shape: (d, 1)\n",
    "coeff_vector_mask_row = jnp.expand_dims(coeff_vector_mask, 0)  # Shape: (1, d)\n",
    "coeff_matrix_mask = jnp.minimum(\n",
    "    coeff_vector_mask_col, coeff_vector_mask_row\n",
    ")  # Shape: (d, d)\n",
    "\n",
    "start_representation = jax.random.normal(jax.random.PRNGKey(0), (32, 11))\n",
    "end_representation = jax.random.normal(jax.random.PRNGKey(1), (32, 11))\n",
    "start_representation_2 = jax.random.normal(jax.random.PRNGKey(2), (32, 11))\n",
    "end_representation_2 = jax.random.normal(jax.random.PRNGKey(3), (32, 11))\n",
    "constraint_start_representation = jax.random.normal(jax.random.PRNGKey(4), (32, 11))\n",
    "constraint_end_representation = jax.random.normal(jax.random.PRNGKey(5), (32, 11))\n",
    "print(start_representation.sum())\n",
    "print(end_representation.sum())\n",
    "print(start_representation_2.sum())\n",
    "print(end_representation_2.sum())\n",
    "print(constraint_start_representation.sum())\n",
    "print(constraint_end_representation.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "db364409",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for approximation error loss: start_representation: (32, 1)\n",
      "for approximation error loss: end_representation: (32, 1)\n",
      "batched approximation error loss\n",
      "batched orthogonality loss\n",
      "for approximation error loss: start_representation: (32, 2)\n",
      "for approximation error loss: end_representation: (32, 2)\n",
      "batched approximation error loss\n",
      "batched orthogonality loss\n",
      "for approximation error loss: start_representation: (32, 3)\n",
      "for approximation error loss: end_representation: (32, 3)\n",
      "batched approximation error loss\n",
      "batched orthogonality loss\n",
      "for approximation error loss: start_representation: (32, 4)\n",
      "for approximation error loss: end_representation: (32, 4)\n",
      "batched approximation error loss\n",
      "batched orthogonality loss\n",
      "for approximation error loss: start_representation: (32, 5)\n",
      "for approximation error loss: end_representation: (32, 5)\n",
      "batched approximation error loss\n",
      "batched orthogonality loss\n",
      "for approximation error loss: start_representation: (32, 6)\n",
      "for approximation error loss: end_representation: (32, 6)\n",
      "batched approximation error loss\n",
      "batched orthogonality loss\n",
      "for approximation error loss: start_representation: (32, 7)\n",
      "for approximation error loss: end_representation: (32, 7)\n",
      "batched approximation error loss\n",
      "batched orthogonality loss\n",
      "for approximation error loss: start_representation: (32, 8)\n",
      "for approximation error loss: end_representation: (32, 8)\n",
      "batched approximation error loss\n",
      "batched orthogonality loss\n",
      "for approximation error loss: start_representation: (32, 9)\n",
      "for approximation error loss: end_representation: (32, 9)\n",
      "batched approximation error loss\n",
      "batched orthogonality loss\n",
      "for approximation error loss: start_representation: (32, 10)\n",
      "for approximation error loss: end_representation: (32, 10)\n",
      "batched approximation error loss\n",
      "batched orthogonality loss\n",
      "for approximation error loss: start_representation: (32, 11)\n",
      "for approximation error loss: end_representation: (32, 11)\n",
      "batched approximation error loss\n",
      "batched orthogonality loss\n",
      "20.428131\n"
     ]
    }
   ],
   "source": [
    "### MUST RUN TOP CELL BEFORE THIS\n",
    "# old computation of loss\n",
    "total_loss = 0\n",
    "total_approximation_error_loss = 0\n",
    "total_orthogonality_loss = 0\n",
    "total_frobenius_norm_loss = 0\n",
    "for top_i in range(1, d + 1):\n",
    "    curr_coef = 1 # d - top_i + 1\n",
    "    curr_loss = _compute_loss_function_component(top_i)\n",
    "    total_loss += curr_coef * curr_loss\n",
    "    \n",
    "print(total_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ab7dbd13",
   "metadata": {},
   "outputs": [],
   "source": [
    "### NEW COMPUTATION\n",
    "def _compute_indexwise_products(f, g):\n",
    "    assert f.shape[0] == g.shape[0]\n",
    "    return jnp.einsum(\"bi, bj -> ij\", f, g) / f.shape[0]\n",
    "\n",
    "def _generate_masks(curr_d):\n",
    "    \"\"\"\n",
    "    Generates a monotonically decreasing mask in dimension index - both vector and matrix\n",
    "    masks. Namely, they look like:\n",
    "    [d, d - 1, ..., 2, 1]\n",
    "    [[3, 2, 1]\n",
    "        [2, 2, 1]\n",
    "        [1, 1, 1]] (for the d = 3 case)\n",
    "\n",
    "    Returns:\n",
    "        Tuple of the form (vector_mask, matrix_mask)\n",
    "    \"\"\"\n",
    "    vector_mask = jnp.arange(curr_d, 0, -1)\n",
    "    matrix_mask = jnp.minimum(\n",
    "        jnp.expand_dims(vector_mask, 1), jnp.expand_dims(vector_mask, 0)\n",
    "    )\n",
    "\n",
    "    return vector_mask, matrix_mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ca5fc13e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GridMaze-7: 11 eigenvalues\n",
      "GridMaze-9: 13 eigenvalues\n",
      "GridMaze-17: 87 eigenvalues\n",
      "GridMaze-19: 161 eigenvalues\n",
      "GridMaze-26: 388 eigenvalues\n",
      "GridMaze-32: 475 eigenvalues\n",
      "GridRoom-1: 225 eigenvalues\n",
      "GridRoom-4: 104 eigenvalues\n",
      "GridRoom-16: 271 eigenvalues\n",
      "GridRoom-32: 544 eigenvalues\n",
      "GridRoom-64: 1088 eigenvalues\n",
      "GridRoomSym-4: 104 eigenvalues\n"
     ]
    }
   ],
   "source": [
    "envs = [\"GridMaze-7\", \"GridMaze-9\", \"GridMaze-17\", \"GridMaze-19\", \"GridMaze-26\", \"GridMaze-32\", \"GridRoom-1\", \"GridRoom-4\", \"GridRoom-16\", \"GridRoom-32\", \"GridRoom-64\", \"GridRoomSym-4\"]\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "for env in envs:\n",
    "    eigval_path = f\"src/env/grid/eigval/{env}.npz\"\n",
    "    try:\n",
    "        data = np.load(eigval_path)\n",
    "        eigvals = data[\"eigval\"] if \"eigval\" in data else data[data.files[0]]\n",
    "        print(f\"{env}: {len(eigvals)} eigenvalues\")\n",
    "    except Exception as e:\n",
    "        print(f\"Could not load eigenvalues for {env}: {e}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
