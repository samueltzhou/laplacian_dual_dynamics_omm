d: 11
n_samples: 5000000 # normally 200000
batch_size: 1024
discount: 0.9
log_dir: '/tmp/rl_laprepr/log'
total_train_steps: 80000 # normally 20000
print_freq: 200
max_episode_steps: 50
seed: 1234
env_name: GridRoom-64
env_family: Grid-v0
lr: 0.001
device: cpu
use_wandb: True
hidden_dims:
  - 256
  - 256
  - 256
activation: relu
use_layer_norm: False # TODO: add support for this
eigval_precision_order: 16
direct_rotation: True
save_eig: False
save_model: False
save_model_every: 50000
do_plot_eigenvectors: True
obs_mode: xy
window_size: 180
reduction_factor: 1
algorithm: al
error_update_rate: 1
q_error_update_rate: 0.1
normalize_graph_loss: False
barrier_initial_val: 2.0
lr_barrier_coefs: 1.0 # learning rate for barrier coefficients, 1.0 in the repository
min_barrier_coefs: 0
max_barrier_coefs: 10000
use_barrier_normalization: False
use_barrier_for_duals: False
lr_duals: 0.0001
lr_dual_velocities: 0.1
min_duals: -100
max_duals: 100
permute_step: 200000 # make this effectively infinite cuz it really messes it up

