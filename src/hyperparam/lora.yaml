d: 11
n_samples: 1000000 # normally 200000
batch_size: 1024
discount: 0.9
log_dir: '/tmp/rl_laprepr/log'
total_train_steps: 40000 # normally 20000
print_freq: 200
save_freq: 10000
max_episode_steps: 50
seed: 1234
env_name: GridRoom-16
env_family: Grid-v0
lr: 0.001
use_lr_schedule: True # new parameter that I added to try and help training stability
final_lr_multiplier: 0.02 # at the end of lr exponential decay, the lr is final_lr_multiplier * lr
use_wandb: True
hidden_dims:
  - 256
  - 256
  - 256
activation: relu
eigval_precision_order: 16
reduction_factor: 1
permute_step: 200000 # make this effectively infinite cuz it really messes it up
direct_rotation: True
save_eig: True
save_model: False
save_model_every: 100000
do_plot_eigenvectors: True
obs_mode: xy
window_size: 128
algorithm: sequential_lora
asymmetric_normalization: False
coefficient_normalization: True
